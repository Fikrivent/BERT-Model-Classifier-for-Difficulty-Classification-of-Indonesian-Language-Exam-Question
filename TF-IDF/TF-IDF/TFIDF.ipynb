{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a4a57e",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58794d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41a1b1",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e76feac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jenis</th>\n",
       "      <th>uraian</th>\n",
       "      <th>pertanyaan</th>\n",
       "      <th>opsi1</th>\n",
       "      <th>opsi2</th>\n",
       "      <th>opsi3</th>\n",
       "      <th>opsi4</th>\n",
       "      <th>kunci_jawaban</th>\n",
       "      <th>kategori</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Makna kata/istilah</td>\n",
       "      <td>Padi yang luas menguning adalah hasil tanam ya...</td>\n",
       "      <td>Makna kata jasa pada paragraf di atas adalah</td>\n",
       "      <td>Pelayanan yang terbaik bagi kita.</td>\n",
       "      <td>Manfaat yang melimpah bagi kita</td>\n",
       "      <td>Perbuatan yang berguna bagi orang lain</td>\n",
       "      <td>Jerih payah yang sangat menguntungkan</td>\n",
       "      <td>Perbuatan yang berguna bagi orang lain</td>\n",
       "      <td>Mudah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antonim/sinonim</td>\n",
       "      <td>Suara radio tetangga sebelah sangat keras pada...</td>\n",
       "      <td>Antonim kata keras pada kalimat tersebut adalah</td>\n",
       "      <td>pelan</td>\n",
       "      <td>lunak</td>\n",
       "      <td>kaku</td>\n",
       "      <td>kencang</td>\n",
       "      <td>pelan</td>\n",
       "      <td>Mudah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Informasi tersurat teks</td>\n",
       "      <td>Pembiasaan Hidup Sehat Sejak KecilPola hidup s...</td>\n",
       "      <td>Bagaimana cara memenuhi gizi anak pada usia ba...</td>\n",
       "      <td>Menambah gizi anak seiring bertambahnya usia a...</td>\n",
       "      <td>Makan makanan bergizi, istirahat cukup, dan ol...</td>\n",
       "      <td>Mempunyai kekebalan yang baik terhadap seranga...</td>\n",
       "      <td>Pemenuhan gizi dengan pemberian ASI saat anak...</td>\n",
       "      <td>Pemenuhan gizi dengan pemberian ASI saat anak...</td>\n",
       "      <td>Sedang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Informasi tersirat teks</td>\n",
       "      <td>Mulai tahun 2016 kuota haji di Indonesssia dit...</td>\n",
       "      <td>Berdasarkan data kuota jumlah jamaah haji untu...</td>\n",
       "      <td>77</td>\n",
       "      <td>77.7</td>\n",
       "      <td>78</td>\n",
       "      <td>77,800</td>\n",
       "      <td>77</td>\n",
       "      <td>Sedang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Informasi tersurat teks</td>\n",
       "      <td>Kebiasaan makan makanan bergizi dengan kadar s...</td>\n",
       "      <td>Kalimat dalam paragraf di atas yang tidak padu...</td>\n",
       "      <td>Kebutuhan gizi anak akan semakin bertambah sei...</td>\n",
       "      <td>Kebutuhan gizi anak baru terpenuhi saat anak b...</td>\n",
       "      <td>Tubuh anak mudah terserang berbagai macam peny...</td>\n",
       "      <td>Pertumbuhan dan perkembangan anak akan semakin...</td>\n",
       "      <td>Tubuh anak mudah terserang berbagai macam peny...</td>\n",
       "      <td>Sedang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     jenis                                             uraian  \\\n",
       "0       Makna kata/istilah  Padi yang luas menguning adalah hasil tanam ya...   \n",
       "1          Antonim/sinonim  Suara radio tetangga sebelah sangat keras pada...   \n",
       "2  Informasi tersurat teks  Pembiasaan Hidup Sehat Sejak KecilPola hidup s...   \n",
       "3  Informasi tersirat teks  Mulai tahun 2016 kuota haji di Indonesssia dit...   \n",
       "4  Informasi tersurat teks  Kebiasaan makan makanan bergizi dengan kadar s...   \n",
       "\n",
       "                                          pertanyaan  \\\n",
       "0       Makna kata jasa pada paragraf di atas adalah   \n",
       "1    Antonim kata keras pada kalimat tersebut adalah   \n",
       "2  Bagaimana cara memenuhi gizi anak pada usia ba...   \n",
       "3  Berdasarkan data kuota jumlah jamaah haji untu...   \n",
       "4  Kalimat dalam paragraf di atas yang tidak padu...   \n",
       "\n",
       "                                               opsi1  \\\n",
       "0                 Pelayanan yang terbaik bagi kita.    \n",
       "1                                             pelan    \n",
       "2  Menambah gizi anak seiring bertambahnya usia a...   \n",
       "3                                                 77   \n",
       "4  Kebutuhan gizi anak akan semakin bertambah sei...   \n",
       "\n",
       "                                               opsi2  \\\n",
       "0                   Manfaat yang melimpah bagi kita    \n",
       "1                                             lunak    \n",
       "2  Makan makanan bergizi, istirahat cukup, dan ol...   \n",
       "3                                               77.7   \n",
       "4  Kebutuhan gizi anak baru terpenuhi saat anak b...   \n",
       "\n",
       "                                               opsi3  \\\n",
       "0            Perbuatan yang berguna bagi orang lain    \n",
       "1                                              kaku    \n",
       "2  Mempunyai kekebalan yang baik terhadap seranga...   \n",
       "3                                                 78   \n",
       "4  Tubuh anak mudah terserang berbagai macam peny...   \n",
       "\n",
       "                                               opsi4  \\\n",
       "0              Jerih payah yang sangat menguntungkan   \n",
       "1                                            kencang   \n",
       "2   Pemenuhan gizi dengan pemberian ASI saat anak...   \n",
       "3                                             77,800   \n",
       "4  Pertumbuhan dan perkembangan anak akan semakin...   \n",
       "\n",
       "                                       kunci_jawaban kategori  label  \n",
       "0            Perbuatan yang berguna bagi orang lain     Mudah      0  \n",
       "1                                             pelan     Mudah      0  \n",
       "2   Pemenuhan gizi dengan pemberian ASI saat anak...   Sedang      1  \n",
       "3                                                 77   Sedang      1  \n",
       "4  Tubuh anak mudah terserang berbagai macam peny...   Sedang      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data\n",
    "dataset = pd.read_csv('dataset.csv', header=0, engine='python')\n",
    "del dataset['No']\n",
    "dataset = dataset.dropna()\n",
    "dataset['kategori'] = dataset['kategori'].astype(\"category\")\n",
    "dataset['label'] = dataset['kategori'].cat.codes\n",
    "dataset = dataset.replace('\\n',' ', regex=True)\n",
    "dataset = dataset.replace('\\t',' ', regex=True)\n",
    "dataset = dataset.replace('\"',' \" ', regex=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ce07e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Makna kata/istilah Padi yang luas menguning ad...\n",
       "1    Antonim/sinonim Suara radio tetangga sebelah s...\n",
       "2    Informasi tersurat teks Pembiasaan Hidup Sehat...\n",
       "3    Informasi tersirat teks Mulai tahun 2016 kuota...\n",
       "4    Informasi tersurat teks Kebiasaan makan makana...\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = dataset['jenis'].map(str) + ' ' + dataset['uraian'].map(str)+ ' ' + dataset['pertanyaan'].map(str)+ ' ' + dataset['opsi1'].map(str)+ ' ' + dataset['opsi2'].map(str)+ ' ' + dataset['opsi3'].map(str)+ ' ' + dataset['opsi4'].map(str)+ ' ' + dataset['kunci_jawaban'].map(str)   # '0' refers to the review text\n",
    "train_y = dataset['label']\n",
    "\n",
    "df_final=[]\n",
    "\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b972a0",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4490e1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fikri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e3dc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fikri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938550f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('indonesian')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729cafb",
   "metadata": {},
   "source": [
    "## Stopwords & Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6be3b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing Dataset\n",
    "for i in range(0, len(df_x)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df_x.iloc[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    df_final.append(review)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d18a5a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tanda baca joya wow alangkah indahnya pemandangan pesisir pantai pasir putih perbaikan kesalahan penggunaan tanda baca kalimat joya terkagum wow alangkah indahnya pemandangan pesisir pantai pasir putih joya terkagum wow alangkah indahnya pemandangan pesisir pantai pasir putih joya terkagum wow alangkah indahnya pemandangan pesisir pantai pasir putih joya terkagum wow alangkah indahnya pemandangan pesisir pantai pasir putih joya terkagum wow alangkah indahnya pemandangan pesisir pantai pasir putih'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[417]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c93fb",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e08de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 418, n_features: 4477\n"
     ]
    }
   ],
   "source": [
    "#tf idf\n",
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(df_final)\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(df_final)\n",
    "## Stopwords\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24ff6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4467</th>\n",
       "      <th>4468</th>\n",
       "      <th>4469</th>\n",
       "      <th>4470</th>\n",
       "      <th>4471</th>\n",
       "      <th>4472</th>\n",
       "      <th>4473</th>\n",
       "      <th>4474</th>\n",
       "      <th>4475</th>\n",
       "      <th>4476</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 4477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  4467  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "413   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "414   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "415   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "416   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "417   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "     4468  4469  4470  4471      4472  4473  4474  4475  4476  \n",
       "0     0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0   0.0  0.052999   0.0   0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0   0.0  0.066326   0.0   0.0   0.0   0.0  \n",
       "..    ...   ...   ...   ...       ...   ...   ...   ...   ...  \n",
       "413   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "414   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "415   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "416   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "417   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[418 rows x 4477 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf\n",
    "\n",
    "test = pd.DataFrame(X_train_tf.toarray())\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235239d",
   "metadata": {},
   "source": [
    "# Generate CSV Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d07df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for evauation successfully generated !\n"
     ]
    }
   ],
   "source": [
    "test['label'] = dataset['label']\n",
    "\n",
    "test.to_csv('dataset_eval_tfidf.csv', index=False, encoding='utf-8')\n",
    "test\n",
    "\n",
    "print(\"Dataset for evauation successfully generated !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f80728",
   "metadata": {},
   "source": [
    "# Train & Test Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe458e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((355, 4477), (355,), (63, 4477), (63,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_eval = dataset = pd.read_csv('dataset_eval_tfidf.csv', header=0, engine='python')\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_eval.drop('label', axis = 1), df_eval['label'], test_size=0.15, random_state=42)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cede3",
   "metadata": {},
   "source": [
    "# Single Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa593aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8095238095238095"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators = 300)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "test_pred = model.predict(test_x)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07a7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted y\n",
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9886771e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Mudah       0.74      0.83      0.78        24\n",
      "      Sedang       0.67      0.62      0.64        13\n",
      "       Sulit       0.96      0.88      0.92        26\n",
      "\n",
      "    accuracy                           0.81        63\n",
      "   macro avg       0.79      0.78      0.78        63\n",
      "weighted avg       0.82      0.81      0.81        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, y_pred, target_names=['Mudah', 'Sedang', 'Sulit']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd3afca",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b6173",
   "metadata": {},
   "source": [
    "## Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48d5283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Eval...   | ACC : 0.8253968253968254 | PREC : 0.8065268065268066 | REC : 0.7905982905982908 | F1-SCORE : 0.7965811965811967 | SUPP : None\n",
      "Fold 2 Eval...   | ACC : 0.8571428571428571 | PREC : 0.8392307692307691 | REC : 0.8301282051282052 | F1-SCORE : 0.8337254901960783 | SUPP : None\n",
      "Fold 3 Eval...   | ACC : 0.8253968253968254 | PREC : 0.7986324786324787 | REC : 0.7905982905982908 | F1-SCORE : 0.7937254901960783 | SUPP : None\n",
      "Fold 4 Eval...   | ACC : 0.8571428571428571 | PREC : 0.8392307692307691 | REC : 0.8301282051282052 | F1-SCORE : 0.8337254901960783 | SUPP : None\n",
      "Fold 5 Eval...   | ACC : 0.8571428571428571 | PREC : 0.8392307692307691 | REC : 0.8301282051282052 | F1-SCORE : 0.8337254901960783 | SUPP : None\n",
      "Fold 6 Eval...   | ACC : 0.8571428571428571 | PREC : 0.8392307692307691 | REC : 0.8301282051282052 | F1-SCORE : 0.8337254901960783 | SUPP : None\n",
      "Fold 7 Eval...   | ACC : 0.8412698412698413 | PREC : 0.8287037037037037 | REC : 0.8173076923076922 | F1-SCORE : 0.8211764705882353 | SUPP : None\n",
      "Fold 8 Eval...   | ACC : 0.8095238095238095 | PREC : 0.7870634920634921 | REC : 0.7895299145299145 | F1-SCORE : 0.787392290249433 | SUPP : None\n",
      "Fold 9 Eval...   | ACC : 0.8095238095238095 | PREC : 0.780982905982906 | REC : 0.7777777777777778 | F1-SCORE : 0.7784615384615385 | SUPP : None\n",
      "Fold 10 Eval...   | ACC : 0.8412698412698413 | PREC : 0.8287037037037037 | REC : 0.8173076923076922 | F1-SCORE : 0.8211764705882353 | SUPP : None\n",
      "Average Accuracy Score(10 Times Evaluation) : 0.838095238095238\n",
      "Average Precision Score(10 Times Evaluation) : 0.8187536167536168\n",
      "Average Recall Score(10 Times Evaluation) : 0.8103632478632479\n",
      "Average F1 Score(10 Times Evaluation) : 0.8133415417449031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "acc_temp = 0\n",
    "prec_temp = 0\n",
    "rec_temp = 0\n",
    "f1_temp = 0\n",
    "n_fold = 10\n",
    "\n",
    "for i in range(n_fold):\n",
    "    model = RandomForestClassifier(n_estimators = 300)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    test_pred = model.predict(test_x)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    temp_res = accuracy_score(test_y, test_pred)\n",
    "    acc_temp+= temp_res\n",
    "    \n",
    "    precision, recall, f_value, support = precision_recall_fscore_support(test_y, test_pred, average='macro')\n",
    "    \n",
    "    prec_temp+=precision\n",
    "    rec_temp += recall\n",
    "    f1_temp += f_value\n",
    "    \n",
    "    print(f'Fold {i+1} Eval...   | ACC : {temp_res} | PREC : {precision} | REC : {recall} | F1-SCORE : {f_value} | SUPP : {support}')\n",
    "\n",
    "res = acc_temp/n_fold\n",
    "prec_res = prec_temp/n_fold\n",
    "rec_res = rec_temp/n_fold\n",
    "f1_res = f1_temp/n_fold\n",
    "\n",
    "print(f'Average Accuracy Score({n_fold} Times Evaluation) : {res}')\n",
    "print(f'Average Precision Score({n_fold} Times Evaluation) : {prec_res}')\n",
    "print(f'Average Recall Score({n_fold} Times Evaluation) : {rec_res}')\n",
    "print(f'Average F1 Score({n_fold} Times Evaluation) : {f1_res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f64a84",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d43a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 2 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 3 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 4 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 5 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 6 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 7 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 8 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 9 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Fold 10 Eval...   | ACC : 0.6190476190476191 | PREC : 0.607843137254902 | REC : 0.5993589743589745 | F1-SCORE : 0.5874258219727346 | SUPP : None\n",
      "Average Accuracy Score(10 Times Evaluation) : 0.619047619047619\n",
      "Average Precision Score(10 Times Evaluation) : 0.6078431372549019\n",
      "Average Recall Score(10 Times Evaluation) : 0.5993589743589745\n",
      "Average F1 Score(10 Times Evaluation) : 0.5874258219727347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "acc_temp = 0\n",
    "prec_temp = 0\n",
    "rec_temp = 0\n",
    "f1_temp = 0\n",
    "n_fold = 10\n",
    "\n",
    "for i in range(n_fold):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    test_pred = model.predict(test_x)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    temp_res = accuracy_score(test_y, test_pred)\n",
    "    acc_temp+= temp_res\n",
    "    \n",
    "    precision, recall, f_value, support = precision_recall_fscore_support(test_y, test_pred, average='macro')\n",
    "    \n",
    "    prec_temp+=precision\n",
    "    rec_temp += recall\n",
    "    f1_temp += f_value\n",
    "    \n",
    "    print(f'Fold {i+1} Eval...   | ACC : {temp_res} | PREC : {precision} | REC : {recall} | F1-SCORE : {f_value} | SUPP : {support}')\n",
    "\n",
    "res = acc_temp/n_fold\n",
    "prec_res = prec_temp/n_fold\n",
    "rec_res = rec_temp/n_fold\n",
    "f1_res = f1_temp/n_fold\n",
    "\n",
    "print(f'Average Accuracy Score({n_fold} Times Evaluation) : {res}')\n",
    "print(f'Average Precision Score({n_fold} Times Evaluation) : {prec_res}')\n",
    "print(f'Average Recall Score({n_fold} Times Evaluation) : {rec_res}')\n",
    "print(f'Average F1 Score({n_fold} Times Evaluation) : {f1_res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1140b6",
   "metadata": {},
   "source": [
    "## SVM Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c992efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 2 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 3 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 4 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 5 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 6 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 7 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 8 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 9 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Fold 10 Eval...   | ACC : 0.6190476190476191 | PREC : 0.6071428571428571 | REC : 0.5950854700854701 | F1-SCORE : 0.5979637646304313 | SUPP : None\n",
      "Average Accuracy Score(10 Times Evaluation) : 0.619047619047619\n",
      "Average Precision Score(10 Times Evaluation) : 0.607142857142857\n",
      "Average Recall Score(10 Times Evaluation) : 0.5950854700854701\n",
      "Average F1 Score(10 Times Evaluation) : 0.5979637646304312\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "acc_temp = 0\n",
    "prec_temp = 0\n",
    "rec_temp = 0\n",
    "f1_temp = 0\n",
    "n_fold = 10\n",
    "\n",
    "for i in range(n_fold):\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    test_pred = model.predict(test_x)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    temp_res = accuracy_score(test_y, test_pred)\n",
    "    acc_temp+= temp_res\n",
    "    \n",
    "    precision, recall, f_value, support = precision_recall_fscore_support(test_y, test_pred, average='macro')\n",
    "    \n",
    "    prec_temp+=precision\n",
    "    rec_temp += recall\n",
    "    f1_temp += f_value\n",
    "    \n",
    "    print(f'Fold {i+1} Eval...   | ACC : {temp_res} | PREC : {precision} | REC : {recall} | F1-SCORE : {f_value} | SUPP : {support}')\n",
    "\n",
    "res = acc_temp/n_fold\n",
    "prec_res = prec_temp/n_fold\n",
    "rec_res = rec_temp/n_fold\n",
    "f1_res = f1_temp/n_fold\n",
    "\n",
    "print(f'Average Accuracy Score({n_fold} Times Evaluation) : {res}')\n",
    "print(f'Average Precision Score({n_fold} Times Evaluation) : {prec_res}')\n",
    "print(f'Average Recall Score({n_fold} Times Evaluation) : {rec_res}')\n",
    "print(f'Average F1 Score({n_fold} Times Evaluation) : {f1_res}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
